#+STARTUP: showeverything
#+AUTHOR:    Donghee Park
# Creative Commons, Share-Alike (cc)
#+EMAIL:     dongheepark@gmail.com
#+HTML_HEAD_EXTRA: <style type="text/css">img {  width: auto ;  max-width: 100% ;  height: auto ;} </style>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://gongzhitaao.org/orgcss/org.css"/>

* 수업
 - 목표: 컴퓨터 비전: OpenCV, Maker Detect, Pose Estimation 에 대해서 이해하고 관련 패키지를 사용할 수 있다.
 - 교재: https://learn.dronemap.io/
 - 코치: 박동희 dongheepark@gmail.com

* 수업 진행
 - 컴퓨터 비전 소개
 - OpenCV
 - ~cv_bridge~
 - Object detection
 - Tag detection

* 수업 자료

** OpenCV 소개
 - 1999 Gary Bradsky 개발 시작
 - 2005 Stanley, 2005 DARPA Grand Challenge
 - OpenCV-3.4.4 2018년 11월 20일 릴리즈
 - https://opencv.org/
 - 튜토리얼: https://docs.opencv.org/3.4.4/
 - 예제: https://docs.opencv.org/3.4.4/examples.html
 - OpenCV 프로젝트 목표

#+BEGIN_QUOTE
Advance vision research by providing not only open but also optimized code for basic vision infrastructure. *No more reinventing the wheel.*

Disseminate vision knowledge by providing a *common infrastructure that developers could build on*, so that code would be more readily readable and transferable.
#+END_QUOTE

** OpenCV 설치

*** C++ (OpenCV-3.4.4)
 - C++11, CMake 3.5.1
 - QR code detector

OpenCV 설치 스크립트(20분 정도)

https://gist.github.com/donghee/15d7272885ac3505e0f20a63678e7ba3

실습: build error 해결

*** 예제
 - /home/donghee/installation/OpenCV-3.4.4/share/OpenCV/samples

** OpenCV 소개

*** Mat: 이미지 타입

[[https://blog.iwanhae.ga/content/images/2015/10/b6df115410caafea291ceb011f19cc4a19ae6c2c.png]]

BGR 순으로 저장.

#+BEGIN_SRC cpp
 // 2x2 행렬, 8비트 저장, 기본값은 빨간색
 Mat M(2,2, CV_8UC3, Scalar(0,0,255));
 cout << "M = " << endl << " " << M << endl << endl;
 imshow("Mat", M);
#+END_SRC

[[https://blog.iwanhae.ga/content/images/2015/10/MatBasicContainerOut1.png]]

[[https://i.imgur.com/vWalF0u.png]]

*** Mat: 이미지 읽기
#+BEGIN_SRC cpp
  Mat image;
  image = imread("qrcode.png");
  imshow("Read image", image);
#+END_SRC

#+BEGIN_SRC cpp
#include "opencv2/highgui/highgui.hpp"

using namespace cv;

int main( int argc, char** argv ) {
  Mat img = imread( argv[1], -1 );
  if( img.empty() ) return -1;
  namedWindow( "Example2", WINDOW_AUTOSIZE );
  imshow( "Example2", img );
  waitKey(0);
  destroyWindow( "Example2" );
}
#+END_SRC

*** Mat: 복사
B는 A의 refernce(헤더만 복사)
C는 A의 clone(헤더, 데이터를 복사)

#+BEGIN_SRC cpp
    Mat A, C;                          // Mat클래스 A와 C를 선언
    A = imread("qrcode.png", IMREAD_COLOR); // imread함수로 이미지"qrcode.png"를 불러옴
    Mat B(A);                                 // B를 선언하고 A를 복사해옴
    C = A.clone();                                    // A의 헤더, 데이터를 카피(클론)하여 C에 저장
#+END_SRC

*** Filter: Sharpen

선명하게(Sharpen) 필터: 현재 위치(i,j)를 변형. 현재 위치 픽셀에 5배 곱하여 더하고, 주변(4방)은 1배 곱하여 뺀다. 현재 위치에 저장.

[[https://blog.iwanhae.ga/content/images/2015/10/7c2c71b792e6560be979d359e8f3f3b34c7938ff.png]]

좌표

[[https://blog.iwanhae.ga/content/images/2015/10/mat.png]]

cam.cpp
#+BEGIN_SRC cpp
#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <iostream>

using namespace cv;
using namespace std;

int main(int argc, char** argv)
{
    VideoCapture vc(0); //0번웹캠 초기화
    if (!vc.isOpened()) return 0; // 연결실패

    Mat img; //img선언
    while (1) {
        vc >> img; //0번웹캠에서 받은 데이터를 img에 저장
        if (img.empty()) break; //받은거 없으면 종료
        imshow("cam", img);  //화면에 띄우기
        if (waitKey(10) == 27) break; //ESC키 눌리면 종료
    }
    destroyAllWindows();
    return 0;
}
#+END_SRC

sharpen.cpp

#+BEGIN_SRC cpp
#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include <iostream>

using namespace cv;
using namespace std;

void Sharpen(const Mat& myImage, Mat& Result)
{
  CV_Assert(myImage.depth() == CV_8U);

  Result.create(myImage.size(), myImage.type());
  const int nChannels = myImage.channels();

  for (int j = 1; j < myImage.rows - 1; ++j)
  {
    const uchar* previous = myImage.ptr<uchar>(j - 1);
    const uchar* current = myImage.ptr<uchar>(j);
    const uchar* next = myImage.ptr<uchar>(j + 1);

    uchar* output = Result.ptr<uchar>(j);

    for (int i = nChannels; i < nChannels * (myImage.cols - 1); ++i)
    {
      *output++ = saturate_cast<uchar>(5 * current[i]
          - current[i - nChannels] - current[i + nChannels] - previous[i] - next[i]);
    }
  }

  Result.row(0).setTo(Scalar(0));
  Result.row(Result.rows - 1).setTo(Scalar(0));
  Result.col(0).setTo(Scalar(0));
  Result.col(Result.cols - 1).setTo(Scalar(0));
}
int main(int argc, char** argv)
{
  VideoCapture vc(0);

  if (!vc.isOpened()) return 0; // 연결실패

  Mat img;
  Mat output;
  Mat kern = (Mat_<char>(3, 3) << 0, -1, 0,
      -1, 5, -1,
      0, -1, 0);

  while (1) {
    vc >> img;

    if (img.empty()) break;
    imshow("cam", img);
    if (waitKey(10) == 27) break; //ESC

    Sharpen(img, output);       //위쪽의 식을 적용시킨 코드
    imshow("output", output);

    filter2D(img, output, img.depth(), kern); //아래쪽 식을 적용시킨 코드
    imshow("output2", output);
  }


  destroyAllWindows();
  return 0;
}

#+END_SRC

----

*** Python (OpenCV-3.4.4)

OpenCV 설치 (Python)
#+BEGIN_SRC sh
pip install opencv-python --user
#+END_SRC

#+BEGIN_SRC py
import cv2
exit()
#+END_SRC

3. camera 읽기

cat > cam.py

#+BEGIN_SRC py
import cv2
cap = cv2.VideoCapture(0)

print 'width: {0}, height: {1}'.format(cap.get(3),cap.get(4))
cap.set(3,320)
cap.set(4,240)

while(True):
    ret, frame = cap.read()

    if (ret):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        cv2.imshow('frame', gray)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()
#+END_SRC

** 실습
**** face detect (힌트 haarcascade)

** ROS 이미지 메시지

 - OpenCV ~cv::Mat~
 - ROS ~sensor_msgs/Image~ http://docs.ros.org/api/sensor_msgs/html/msg/Image.html
#+BEGIN_SRC
rosmsg info sensor_msgs/Image
#+END_SRC

** ~cv_bridge~

ROS에서의 OpenCV 인터페이스

[[http://wiki.ros.org/cv_bridge/Tutorials/UsingCvBridgeCppHydro?action=AttachFile&do=get&target=cvbridge4.png]]

*** CvImage

#+BEGIN_SRC c++
namespace cv_bridge {

class CvImage
{
public:
  std_msgs::Header header;
  std::string encoding;
  cv::Mat image;
};

typedef boost::shared_ptr<CvImage> CvImagePtr;
typedef boost::shared_ptr<CvImage const> CvImageConstPtr;

}
#+END_SRC

** 예시: ~image_converter~

#+BEGIN_SRC
cd ~/catkin_ws/src
catkin_create_pkg cv_test sensor_msgs cv_bridge roscpp rospy std_msgs image_transport
#+END_SRC

#+BEGIN_SRC
cd cv_test/src
#+END_SRC

cat > ~image_converter.cpp~
#+BEGIN_SRC
#include <ros/ros.h>
#include <image_transport/image_transport.h>
#include <cv_bridge/cv_bridge.h>
#include <sensor_msgs/image_encodings.h>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>

static const std::string OPENCV_WINDOW = "Image window";

class ImageConverter
{
  ros::NodeHandle nh_;
  image_transport::ImageTransport it_;
  image_transport::Subscriber image_sub_;
  image_transport::Publisher image_pub_;

public:
  ImageConverter()
    : it_(nh_)
  {
    // Subscrive to input video feed and publish output video feed
    image_sub_ = it_.subscribe("/cv_camera/image_raw", 1,
      &ImageConverter::imageCb, this);
    image_pub_ = it_.advertise("/image_converter/output_video", 1);

    cv::namedWindow(OPENCV_WINDOW);
  }

  ~ImageConverter()
  {
    cv::destroyWindow(OPENCV_WINDOW);
  }

  void imageCb(const sensor_msgs::ImageConstPtr& msg)
  {
    cv_bridge::CvImagePtr cv_ptr;
    try
    {
      cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);
    }
    catch (cv_bridge::Exception& e)
    {
      ROS_ERROR("cv_bridge exception: %s", e.what());
      return;
    }

    // Draw an example circle on the video stream
    if (cv_ptr->image.rows > 60 && cv_ptr->image.cols > 60)
      cv::circle(cv_ptr->image, cv::Point(50, 50), 10, CV_RGB(255,0,0));

    // Update GUI Window
    cv::imshow(OPENCV_WINDOW, cv_ptr->image);
    cv::waitKey(3);

    // Output modified video stream
    image_pub_.publish(cv_ptr->toImageMsg());
  }
};

int main(int argc, char** argv)
{
  ros::init(argc, argv, "image_converter");
  ImageConverter ic;
  ros::spin();
  return 0;
}
#+END_SRC

~~/catkin_ws/src/cv_test/CMakeLists.txt~ 에 다음 내용 추가
 - ~image_converter~ 노드 컴파일 추가
 - OpenCV 라이브러리 추가

#+BEGIN_SRC
set(OpenCV_DIR /home/donghee/installation/OpenCV-3.4.4/share/OpenCV/)
find_package( OpenCV REQUIRED )

set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED TRUE)

include_directories(include ${OpenCV_INCLUDE_DIRS})

include_directories(include ${catkin_INCLUDE_DIRS})
add_executable(image_converter src/image_converter.cpp)
target_link_libraries(image_converter ${catkin_LIBRARIES})

target_link_libraries(image_converter ${OpenCV_LIBS})
#+END_SRC

#+BEGIN_SRC
cd ~/catkin_ws
catkin build cv_test
#+END_SRC

** 예시: ~image_converter~ 노드 실행

*cv-camera, image-view 설치*

#+BEGIN_SRC
sudo apt-get install ros-melodic-cv-camera
sudo apt-get install ros-melodic-image-view
#+END_SRC

*ros master node 실행*
#+BEGIN_SRC
roscore
#+END_SRC

*~cv_camera_node~ 실행*
#+BEGIN_SRC
rosrun cv_camera cv_camera_node
#+END_SRC

*~image_converter~ 실행*
#+BEGIN_SRC
rosrun cv_test image_converter
#+END_SRC

*토픽 목록 보기*
#+BEGIN_SRC
rostopic list -v
#+END_SRC

*** 구독 ~/cv_camera/image_raw~ 이미지 보기
image converter 에서 구독(subscribe) 하는 토픽 이미지 확인

#+BEGIN_SRC c++
rosrun image_view image_view image:=/cv_camera/image_raw
// or using rqt
rqt_image_view /cv_camera/image_raw
#+END_SRC

*** 발행 ~/image_converter/output_video~ 이미지 보기
image converter 에서 publish 하는 토픽 이미지 확인

#+BEGIN_SRC
rosrun image_view image_view image:=/image_converter/output_video
// or using rqt
rqt_image_view /image_converter/output_video
#+END_SRC

*rviz*

#+BEGIN_SRC
rviz
#+END_SRC

[[https://i.imgur.com/jsulKek.png]]

*** 실습
 - ~/image_converter/output_video~ 토픽의 이미지에 circle 좌표(x,y)를 표시해보자.

** 예시: object detection: ~dnn_detect~

[[https://i.imgur.com/GKDEe3r.png]]

MobileNet-SSD detection: 딥러닝

ros kinetic 버전에서만 동작

#+BEGIN_SRC
sudo apt install ros-kinetic-dnn-detect
#+END_SRC

#+BEGIN_SRC
roscore
rosrun cv_camera cv_camera_node
roslaunch dnn_detect dnn_detect.launch camera:=/cv_camera image:=image_raw
rostopic echo /dnn_objects
rosrun image_view image_view image:=/dnn_images
#+END_SRC

#+BEGIN_SRC
rostopic list -v
#+END_SRC

** 예시: tag detection: aruco

aruco: library for detect marker https://sourceforge.net/projects/aruco/
 1. detect marker
 2. position estimation

[[https://i.imgur.com/pa6a5HY.png]]

*aruco-ros 설치* ros melodic 버전
#+BEGIN_SRC
cd ~/catkin_ws/src/
git clone https://github.com/pal-robotics/aruco_ros
cd ~/catkin_ws
catkin build aruco_ros
#+END_SRC

*ros-kinetic-aruco-ros 설치* ros kinetic 버전
#+BEGIN_SRC
sudo apt-get install ros-kinetic-aruco-ros
#+END_SRC

~aruco_test.launch~ 작성
#+BEGIN_SRC xml
  <launch>

  <arg name="markerId" default="701"/>
  <arg name="markerSize" default="0.05"/> <!-- in meter -->
  <arg name="eye" default="left"/>
  <arg name="marker_frame" default="marker_frame"/>
  <arg name="ref_frame" default=""/> <!-- leave empty and the pose will be published wrt param parent_name -->
  <arg name="corner_refinement" default="LINES" /> <!-- NONE, HARRIS, LINES, SUBPIX -->

  <node pkg="aruco_ros" type="single" name="aruco_single">
  <remap from="/camera_info" to="/cv_camera/camera_info" />
  <remap from="/image" to="/cv_camera/image_raw" />
  <param name="image_is_rectified" value="True"/>
  <param name="marker_size" value="$(arg markerSize)"/>
  <param name="marker_id" value="$(arg markerId)"/>
  <param name="reference_frame" value="$(arg ref_frame)"/> <!-- frame in which the marker pose will be refered -->
  <param name="camera_frame" value="base_link"/>
  <param name="marker_frame" value="$(arg marker_frame)" />
  <param name="corner_refinement" value="$(arg corner_refinement)" />
  <!-- <param name="calibration_file" type="string" value="/tmp/ost.yaml"/> -->
  </node>

  </launch>
#+END_SRC

*aruco 노드 실행*
#+BEGIN_SRC
roslaunch aruco_test.launch
#+END_SRC

*마커 결과 확인*
#+BEGIN_SRC
rosrun image_view image_view image:=/aruco_single/result
#+END_SRC

** 예시: position estimation using aruco

[[https://i.imgur.com/arFC1S0.png]]

*마커 위치/자세 확인*
#+BEGIN_SRC
rostopic echo /aruco_single/pose
#+END_SRC

*** Camera Calibration

http://wiki.ros.org/camera_calibration
http://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration

*camera calibration 실행*
#+BEGIN_SRC
rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.025 image:=/cv_camera/image_raw camera:=/cv_camera
#+END_SRC

calibration 후 commit 버튼 클릭

~cv_camera_node~ 다시 실행하여 camera calibration 데이터 적용

* 참고
 - http://wiki.ros.org/vision_opencv
 - http://wiki.ros.org/cv_bridge/Tutorials/UsingCvBridgeToConvertBetweenROSImagesAndOpenCVImages
  - http://wiki.ros.org/image_transport/Tutorials/PublishingImages
  - http://wiki.ros.org/image_transport/Tutorials/SubscribingToImages
 - ArUco Library Documentation https://docs.google.com/document/d/1QU9KoBtjSM2kF6ITOjQ76xqL7H0TEtXriJX5kwi9Kgc/
 - aruco maker generator http://chev.me/arucogen/
 - http://wiki.ros.org/dnn_detect
  - mobilenet-ssd https://github.com/weiliu89/caffe/tree/ssd

----
** TF

TF(TransForm 프레임 변환)

왜? 카메라에서 나오는 x_offset, y_offset 좌표를 드론의 BODY(local position) 좌표계로 변경 필요

*** TF?

http://wiki.ros.org/navigation/Tutorials/RobotSetup/TF?action=AttachFile&do=get&target=simple_robot.png

Listener: /tf 토픽을 읽음
Publisher: 좌표간의 *변환* 을 /tf에 방송(publish)

*** TF의 xyz ?

 x - forward
 y - left
 z - pointing up

출처: http://www.ros.org/reps/rep-0103.html

*PX4 local position은 NED*

** TF의 lookupTransform

[[https://i.imgur.com/gsHe8gx.png]]


*** TF 실습

터틀심의 /turtle1을 따라다니는 turtle2를 만들어 보자

http://wiki.ros.org/tf2/Tutorials

따라하기

https://www.youtube.com/watch?v=aCH259ggKb0

참고
 - http://wiki.ros.org/tf
 - http://wiki.ros.org/navigation/Tutorials/RobotSetup/TF
 - https://github.com/claymation/lander/
 - http://web.ics.purdue.edu/~rvoyles/Classes/ROSprogramming/Lectures/TF%20(transform)%20in%20ROS.pdf
 - https://www.ethz.ch/content/dam/ethz/special-interest/mavt/robotics-n-intelligent-systems/rsl-dam/ROS2017/lecture3.pdf

----

https://github.com/CopterExpress/clover/tree/master/aruco_pose

* 참고
 - https://clover.coex.tech/en
